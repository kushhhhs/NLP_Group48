{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import nltk, string, random, numpy, os\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List\n",
    "os.makedirs('./results', exist_ok=True)\n",
    "nltk.download('brown')\n",
    "random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "numpy.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "In the warm-up of this assignment, we will see how Neural Networks (NN) handle natural language data.   \n",
    "The warm-up focuses on a simple Multilayer Perceptron (MLP), also known as a fully connected Neural Network. The data we'll use is the first 5000 unique words of the Brown corpus.\n",
    "\n",
    "### Dataset\n",
    "To train the model, we will have to represent the input words to the model in some way. Since models solely work with numbers, the words will have to be converted into numerical form.  \n",
    "For this assignment, we will focus on predicting individual words from the dataset given the input of the model. The input will be the target word split up into individual letters. To represent these individual letters we will give the model a vector of 26 positions (26 letters in the English alphabet). Initially, this vector is filled with zeros and for every occuring letter in the word we change the value to 1 in that position. For instance, in the word `apple', we have 1 a, 1 e, 1 l and 2 p. The vector will then represent the word as:  \n",
    "```[1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,2,0,0,0,0,0,0,0,0,0,0]```  \n",
    "You will have to implement this algorithm together with loading the Brown dataset and taking the first 5000 unique words. Implement a way to store the indexes of the unique words as a dictionary where the word is the key and the index is the value as well as the target list (which will be just the indexes of the words)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlphaDataset(Dataset):\n",
    "    def __init__(self) -> None:\n",
    "        # Load the NLTK Brown corpus and store the first 5000 unique words of the corpus in self.data\n",
    "        self.data = None\n",
    "        self.word_to_idx = None # Convert the unique words to an index dictionary {word: index}\n",
    "        self.targets = None # Make these indexes the target values\n",
    "        \n",
    "        corpus = nltk.corpus.brown.words()\n",
    "\n",
    "        # use a set to only keep unique words\n",
    "        self.data = list(set(corpus))[:5000] \n",
    "        self.data = [word.lower() for word in self.data]\n",
    "        \n",
    "        self.word_to_idx = {word: index for index, word in enumerate(self.data)}\n",
    "        self.targets = [self.word_to_idx[word] for word in self.data]\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        # Torch requires the implementation of the length function to calculate the number of instances in the dataset. Find a way to implement this\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = self.data[index]\n",
    "        y = self.targets[index]\n",
    "\n",
    "        # Apply processing to turn the word (stored in x) into a numeric vector of 26 numbers, counting the occurences of the letters.\n",
    "        # Example: apple would become [1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,2,0,0,0,0,0,0,0,0,0,0], counting 2 occurences for the letter p and zero for letters that do not occur.\n",
    "        \n",
    "        word = x\n",
    "        x = [0] * 26\n",
    "\n",
    "        for letter in word:\n",
    "            # since x has 26 slots, only for letters a-z we check the letter is actually in the alphabet (not a number or punctuation for example)\n",
    "            if letter.isalpha(): \n",
    "\n",
    "                # I found this function that returns the integer representing a unicode character\n",
    "                # for ord('a'), this is 97, so we need to subtract this from ord(letter) to get the actual index in the alphabet\n",
    "                # https://docs.python.org/3.4/library/functions.html#ord\n",
    "                x[ord(letter) - ord('a')] += 1\n",
    "                \n",
    "        x = torch.tensor(x, dtype=torch.float32)\n",
    "        y = torch.tensor(y, dtype=torch.long)\n",
    "        \n",
    "        return x, y\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a test cell to understand how the ord() function works and if the output is consistent with the example given in the comment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numeric_vector(word):\n",
    "    \"\"\"\n",
    "    Convert a word to a numeric vector of 26 numbers, counting the occurrences of the letters.\n",
    "    This function is used to test whether the implementation is correct.\n",
    "    \"\"\"\n",
    "    word = word.lower()\n",
    "    x = [0] * 26\n",
    "\n",
    "    for letter in word:\n",
    "        if letter.isalpha():\n",
    "            print(f\"Unicode integer for {letter}: {ord(letter)}\")\n",
    "            print(f\"Unicode integer for 'a': {ord('a')}\")\n",
    "            print(f\"Index in alphabet for {letter}: {ord(letter) - ord('a')}\")\n",
    "\n",
    "            x[ord(letter) - ord('a')] += 1\n",
    "            \n",
    "    x = torch.tensor(x, dtype=torch.float32)\n",
    "\n",
    "    return x\n",
    "\n",
    "# Example: apple would become [1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,2,0,0,0,0,0,0,0,0,0,0], counting 2 occurences for the letter p and zero for letters that do not occur.\n",
    "print(numeric_vector('Apple'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multilayer Perceptron\n",
    "In the following section, you will implement a MLP. The goal is to implement this MLP with 1 input layer, 2 hidden layers, and 1 output layer.\n",
    "With PyTorch, the linear layer is most suitable for this. When you create a linear layer, you define the input and output size of the layer, effectively creating two linear neuron layers. This is useful to know since we only need to create 3 linear layer classes to have the 4 layers we want.\n",
    "The hidden size is stored as a list where the first value will be 256 and the second value will be 512."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size: int, hidden_size: List[int], output_size: int) -> None:\n",
    "        super().__init__()\n",
    "        # Implement the neural network layers, the activation function is already defined\n",
    "        self.input_layer = nn.Linear(input_size, hidden_size[0])\n",
    "        self.hidden_layer = nn.Linear(hidden_size[0], hidden_size[1])\n",
    "        self.output_layer = nn.Linear(hidden_size[1], output_size)\n",
    "\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # In the forward pass the model will calculate the gradients as well as the probabilities of the result occuring given its input.\n",
    "        # Implement the missing layers\n",
    "\n",
    "        x = self.input_layer(x) # Implement input layer\n",
    "        x = self.activation(x)\n",
    "        x = self.hidden_layer(x) # Implement hidden layer\n",
    "        x = self.activation(x)\n",
    "        x = self.output_layer(x) # Implement the output layer\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the hyperparameters\n",
    "These are the hyperparameters used for the model, they define the layout of the model as well as the performance:\n",
    "- batch_size, defines the number of instances the model sees at one time.\n",
    "- learning_rate, defines the change rate of the gradient descent.\n",
    "- input_size, the number of input neurons for the model, the number of letters in the alphabet\n",
    "- hidden_size, the number of neurons in the hidden layer\n",
    "- output_size, the number of neurons in the output layer, for us this is the number of unqiue words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "learning_rate = 1e-5\n",
    "input_size = 26\n",
    "hidden_size = [256, 512]\n",
    "output_size = 5000\n",
    "device = 'cpu' # If you have an m1 macbook use: 'mbp', if you have an NVIDIA GPU use: 'cuda:0' else leave as is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dataset and the dataloader\n",
    "dataset = AlphaDataset()\n",
    "\n",
    "# For the final evaluation of the model we will use 20% of the data for testing. Testing is only ever done after hyperparameter tuning.\n",
    "# Split sizes (80% train, 20% test)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# To evaluate our model we want to take 10% of the dataset for validation, this is similar to the testset, rather this data we can use during hyperparameter tuning.\n",
    "# The validation and test data is never trained on and is unseen data for the model, making it closer to a production setting.\n",
    "train_size = int(0.9 * len(train_dataset))\n",
    "val_size = len(train_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True) # Shuffling ensures the model does not overfit on ordering of the data.\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False) # This data does not need to be shuffled\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)  #T his data does not need to be shuffled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preperation for training\n",
    "Here we load the model into memory, apply it to the selected device and define the optimizer. The optimizer guides the model to the best possible state it can be in through Gradient descent.\n",
    "Lastly, the loss function is defined, this defines how well the model performs, based on this number the model knows how it should change its weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP(input_size, hidden_size, output_size)\n",
    "model.to(device) # Tell de model which accelerator to use (Macbook GPU, NVIDIA GPU or CPU)\n",
    "\n",
    "# In Neural Networks optimizers handle the efficient training through gradient descent, we will use Adam\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=learning_rate)\n",
    "\n",
    "# The loss function defines how well the model is performing, if the loss is low the model is rewarded, if it is high the model is punished.\n",
    "# Since we are dealing with a classification task we will use Cross Entropy\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "In the next block the training block is already defined. This is a standard way to train the model for 50 epochs (50 times it will see the dataset). Each time it does one epoch we also go over the validationset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_losses = []\n",
    "validation_losses = []\n",
    "\n",
    "for epoch in range(50): # Train for 50 epochs\n",
    "    model.train() # Enforce model training\n",
    "    for step, (x, y) in enumerate(train_loader):\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        output = model(x)\n",
    "\n",
    "        loss = loss_fn(output, y)\n",
    "\n",
    "        training_losses.append(loss.item())\n",
    "        print(loss.item(), end='\\r')\n",
    "\n",
    "        loss.backward() # Calculate gradients\n",
    "\n",
    "        optimizer.step() # Reward the model\n",
    "        optimizer.zero_grad() # Clean the gradients\n",
    "    print('Training_loss:', loss.item())\n",
    "\n",
    "    model.eval() # After every training epoch we want to see the model's performance on the validation data\n",
    "    with torch.no_grad(): # In validation we dont need gradients so we tell torch to not calculate them\n",
    "        total_val_loss = 0\n",
    "        for step, (x, y) in enumerate(val_loader):\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            output = model(x)\n",
    "\n",
    "            loss = loss_fn(output, y)\n",
    "\n",
    "            total_val_loss += loss.item()\n",
    "\n",
    "        validation_losses.append(total_val_loss / len(val_dataset))\n",
    "        print('Validation_loss:', validation_losses[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing\n",
    "Below we can print the training statistics, the training loss should be going down while the validation loss should be going up. What does it mean that the validation loss increases?\n",
    "\n",
    "If the validation loss increases, this means there is overfit on the training data in the model. This means that the model is modelling the training data too closely, where it is not just picking up general patterns that will generalize to other data well, but it is also picking up on the specific patterns unique to the training data. \n",
    "\n",
    "For our specific case, this probably means that the model is learning to predict words that are in the training data, since this is the only data it can learn from. But for the validation data, this means that the model incorrectly predicts words, since the training words are not in the validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.dpi'] = 150\n",
    "plt.plot(training_losses)\n",
    "plt.xlabel('steps')\n",
    "plt.ylabel('loss')\n",
    "plt.title('Training Loss')\n",
    "plt.savefig('./results/training_loss.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(validation_losses)\n",
    "plt.xlabel('steps')\n",
    "plt.ylabel('loss')\n",
    "plt.title('Validation Loss')\n",
    "plt.savefig('./results/validation_loss.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing\n",
    "Belowe we want to analyze how the model functions based on the test data. What stands out from these results? Was the result correct? What is the main difference between the words?\n",
    "\n",
    "Looking at the first 10 words predicted by our model from the test data, a couple of observations can be made:\n",
    "1. **All words are predicted incorrectly**. This can probably be attributed to the observations made in the question above; the model is overfitting on the training data. It has only ever seen the training words get correctly predicted, so it has learned to predict these words to be correct.\n",
    "\n",
    "2. **The target and predicted words are similar in letters**. When you look at targets en predictions like 'earmarked' and 'Kader', 'Orwell' and 'lower', or 'shift' and 'fish', it is clear that there is significant overlap in the letters between these words. This indicates that the model *has* in fact learned something that we were trying to achieve; it is able to map a numerical vector representation to the corresponding letters. \n",
    "However, since this representation does not include any positional information, the model could never learn the position of these letters in the words.\n",
    "\n",
    "3. **All predicted words are from the training data**. This again goes back to the overfitting problem; the model has only ever learned that the training words result in a correct prediction, and since there is not overlap between training and testing words, it is just predicting the training words that are similar in letters to the testing words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_n_words(dataset, n, model, ds, verbose = True):\n",
    "    if n == -1:\n",
    "        n = len(dataset)\n",
    "    x = [dataset.__getitem__(i)[0] for i in range(n)]\n",
    "    y = [dataset.__getitem__(i)[1] for i in range(n)]\n",
    "    target_words = [{idx: word for word, idx in ds.word_to_idx.items()}[_.item()] for _ in y]\n",
    "    out = [nn.functional.softmax(model(wrd)).argmax() for wrd in x]\n",
    "    predicted_words = [{idx: word for word, idx in ds.word_to_idx.items()}[_.item()] for _ in out]\n",
    "\n",
    "    # add word vectors to ouptput so I can use it later for similarities\n",
    "    target_vectors = [ds.__getitem__(ds.word_to_idx[_])[0] for _ in target_words]\n",
    "    pred_vectors = [ds.__getitem__(ds.word_to_idx[_])[0] for _ in predicted_words]\n",
    "\n",
    "\n",
    "    width = max(len(word) for word in target_words) + 5\n",
    "    if verbose:\n",
    "        print('\\n'.join([f'target: {t.ljust(width)} predicted: {p}' for t, p in zip(target_words, predicted_words)]))\n",
    "    return target_words, predicted_words, target_vectors, pred_vectors\n",
    "\n",
    "targets, preds, target_vectors, pred_vectors = get_n_words(test_dataset, 10, model, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_words = [{idx: word for word, idx in dataset.word_to_idx.items()}[_.item()] for _ in [test_dataset.__getitem__(i)[1] for i in range(len(test_dataset))]]\n",
    "train_words = [{idx: word for word, idx in dataset.word_to_idx.items()}[_.item()] for _ in [train_dataset.__getitem__(i)[1] for i in range(len(train_dataset))]]\n",
    "\n",
    "for word in preds:\n",
    "    if word in test_words:\n",
    "        print(f\"The predicted word '{word}' is in the test set.\")\n",
    "    if word in train_words:\n",
    "        print(f\"The predicted word '{word}' is in the train set.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why are some words incorrectly predicted?\n",
    "\n",
    "As mentioned in previous questions, the model is clearly overfitting on the training data. In short, the model is learning to predict the words that are in the training data. Since the words in the validation data and test data are by definition different (we chose 5000 unique words), the model never learns to predict these words.\n",
    "\n",
    "Illustrating this:\n",
    "\n",
    "The model has 5000 outputs; 1 for every unique word we extracted from the corpus. Let's say the training data contains 3600 of these unique words. During training, the model gets different numerical vectors as input and learns which inputs map to which output node. Since we only have 3600 out of 5000 words in the training data, the model never receives any input that leads to any of the other 1400 output nodes to be correct. The model also has no 'idea' what these output nodes represent, it just learns that certain inputs map to certain outputs. It can thus never learn that the number of letters in the input has to match the number of letters in the output; it has no 'idea' which letters are in the output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code + Written\n",
    "Research the Jaccard similarity metric for calculating the difference between the predicted word and the target word. Reference your sources and implement this metric in your code below. You can use the get_n_words function with n=-1 to get all the predicted and target words. Compare this method to similarity as measured with one of the word vector methods from A2, in writing and/or code.\n",
    "\n",
    "[Your answer here]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_similarity(targets, predictions):\n",
    "    if len(targets) != len(predictions):\n",
    "        raise ValueError(\"The lengths of the two lists must be equal.\")\n",
    "    \n",
    "    similarities = []\n",
    "\n",
    "    for i in range(len(targets)):\n",
    "        target_set = set(targets[i])\n",
    "        prediction_set = set(predictions[i])\n",
    "\n",
    "        similarities.append(len(target_set & prediction_set) / len(target_set | prediction_set))\n",
    "\n",
    "    return similarities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets, preds, _, _ = get_n_words(test_dataset, -1, model, dataset, verbose=False)\n",
    "similarities = jaccard_similarity(targets, preds)\n",
    "\n",
    "width = max(len(word) for word in targets) + 2\n",
    "\n",
    "for i in range(len(targets)):\n",
    "    print('\\n'.join([f'target: {t.ljust(width)} predicted: {p.ljust(width)} similarity: {s}' for t, p, s in zip(targets, preds, similarities)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def cosine_similarity(target_vectors, pred_vectors):\n",
    "    if len(target_vectors) != len(pred_vectors):\n",
    "        raise ValueError(\"The lengths of the two lists must be equal.\")\n",
    "    \n",
    "    similarities = []\n",
    "\n",
    "    for i in range(len(target_vectors)):\n",
    "        similarities.append(np.dot(target_vectors[i], pred_vectors[i]) / (np.linalg.norm(target_vectors[i]) * np.linalg.norm(pred_vectors[i])))\n",
    "\n",
    "    return similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets, preds, target_vectors, pred_vectors = get_n_words(test_dataset, -1, model, dataset, verbose=False)\n",
    "similarities = cosine_similarity(target_vectors, pred_vectors)\n",
    "\n",
    "width = max(len(word) for word in targets) + 2\n",
    "\n",
    "for i in range(len(targets)):\n",
    "    print('\\n'.join([f'target: {t.ljust(width)} predicted: {p.ljust(width)} similarity: {s}' for t, p, s in zip(targets, preds, similarities)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
